{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15536836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of HumanEval problems: 164\n",
      "Example task keys: dict_keys(['task_id', 'prompt', 'entry_point', 'canonical_solution', 'test'])\n",
      "\n",
      "Prompt:\n",
      " from typing import List\n",
      "\n",
      "\n",
      "def separate_paren_groups(paren_string: str) -> List[str]:\n",
      "    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n",
      "    separate those group into separate strings and return the list of those.\n",
      "    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n",
      "    Ignore any spaces in the input string.\n",
      "    >>> separate_paren_groups('( ) (( )) (( )( ))')\n",
      "    ['()', '(())', '(()())']\n",
      "    \"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from human_eval.data import read_problems, write_jsonl\n",
    "\n",
    "problems = read_problems()\n",
    "print(\"Number of HumanEval problems:\", len(problems))\n",
    "\n",
    "# Inspect one problem\n",
    "example_task = \"HumanEval/1\"\n",
    "print(\"Example task keys:\", problems[example_task].keys())\n",
    "print(\"\\nPrompt:\\n\", problems[example_task][\"prompt\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c50fa6",
   "metadata": {},
   "source": [
    "## HELPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46b4343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import time\n",
    "import tempfile\n",
    "import subprocess\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def get_failed_results(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSONL results file and extracts all entries where 'passed' is False.\n",
    "    \"\"\"\n",
    "    failed_results = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            if not data.get(\"passed\", False):\n",
    "                failed_results.append(data)\n",
    "    return failed_results\n",
    "\n",
    "\n",
    "def extract_clean_code(generated_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts and cleans Python code from an AI-generated response.\n",
    "    \"\"\"\n",
    "    code_blocks = re.findall(r\"```(?:python)?\\s*(.*?)\\s*```\", generated_text, re.DOTALL)\n",
    "    if code_blocks:\n",
    "        return max(code_blocks, key=len).strip()\n",
    "    \n",
    "    lines = generated_text.splitlines()\n",
    "    code_started = False\n",
    "    code_lines = []\n",
    "    for line in lines:\n",
    "        if not code_started and line.lstrip().startswith(('def ', 'class ', 'import ', 'from ')):\n",
    "            code_started = True\n",
    "        if code_started:\n",
    "            code_lines.append(line)\n",
    "    if code_lines:\n",
    "        return \"\\n\".join(code_lines).strip()\n",
    "    \n",
    "    return generated_text.strip()\n",
    "\n",
    "\n",
    "def strip_function_def(prompt: str, generated_code: str) -> str:\n",
    "    \"\"\"\n",
    "    Robustly removes the function signature and imports to prevent nesting errors.\n",
    "    \"\"\"\n",
    "    # 1. Extract the function name from the prompt to know what to look for\n",
    "    # (Assuming standard HumanEval prompt format ending in 'def func_name(...):')\n",
    "    func_name_match = re.search(r\"def\\s+(\\w+)\\s*\\(\", prompt)\n",
    "    func_name = func_name_match.group(1) if func_name_match else None\n",
    "\n",
    "    lines = generated_code.splitlines()\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    # 2. Skip lines until we pass the function definition\n",
    "    inside_function = False\n",
    "    \n",
    "    for line in lines:\n",
    "        # If we see the function definition again, ignore it and toggle flag\n",
    "        if func_name and f\"def {func_name}\" in line:\n",
    "            inside_function = True\n",
    "            continue # Skip the signature line itself\n",
    "        \n",
    "        # If we haven't found the def yet, and the line is an import or comment, keep it\n",
    "        # BUT standard HumanEval usually expects indented code immediately.\n",
    "        # Safer strategy: Only keep lines that are indented (the body)\n",
    "        if line.startswith(\" \") or line.startswith(\"\\t\"):\n",
    "            cleaned_lines.append(line)\n",
    "            \n",
    "    # Fallback: if the sophisticated strip failed, use the simple one or raw\n",
    "    if not cleaned_lines:\n",
    "        return generated_code\n",
    "\n",
    "    return \"\\n\".join(cleaned_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea5426f",
   "metadata": {},
   "source": [
    "## BASIC CODE GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3d7fcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import tqdm\n",
    "\n",
    "def generate_one_completion_basic(prompt: str, model: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses local Ollama to generate a HumanEval-compatible completion.\n",
    "    Returns ONLY the function body (indented).\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"http://127.0.0.1:11434/api/generate\"\n",
    "\n",
    "    system_msg = (\n",
    "        \"You are a Python coding assistant.\\n\"\n",
    "        \"Complete the function below.\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"- Output ONLY Python code\\n\"\n",
    "        \"- Do NOT repeat the function signature\\n\"\n",
    "        \"- Do NOT add comments or explanations\\n\"\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": system_msg + \"\\n\\n\" + prompt,\n",
    "        \"temperature\": 0.1,\n",
    "        \"num_predict\": 400,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(url, json=payload, timeout=300)\n",
    "        resp.raise_for_status()\n",
    "        raw = resp.json().get(\"response\", \"\").strip()\n",
    "\n",
    "        if not raw:\n",
    "            raise RuntimeError(\"Empty response from Ollama\")\n",
    "\n",
    "        cleaned = extract_clean_code(raw)\n",
    "        print (\"----------------------------------\")\n",
    "        print (f\"Raw generated code:\\n{raw}\\n\")\n",
    "        print(\"-------------------------------------\")\n",
    "        print (f\"Cleaned generated code:\\n{cleaned}\\n\")\n",
    "        print (\"----------------------------------\")\n",
    "\n",
    "        completion = strip_function_def(prompt, cleaned)\n",
    "        print (f\"Final completion after stripping function def:\\n{completion}\\n\")\n",
    "        return completion\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"[Generation error]\", e)\n",
    "        return \"    pass\"\n",
    "\n",
    "\n",
    "def generate_pass_k_samples(\n",
    "    problems,\n",
    "    model_name: str,\n",
    "    k: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates k independent samples per task (pass@k).\n",
    "    \"\"\"\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    for task_id in tqdm(problems, desc=f\"Generating pass@{k} samples\"):\n",
    "        prompt = problems[task_id][\"prompt\"]\n",
    "\n",
    "        for _ in range(k):\n",
    "            completion = generate_one_completion_basic(\n",
    "                prompt,\n",
    "                model_name\n",
    "            )\n",
    "\n",
    "            samples.append({\n",
    "                \"task_id\": task_id,\n",
    "                \"completion\": completion\n",
    "            })\n",
    "\n",
    "    return samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a213467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pass@1 samples:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:01<00:03,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Raw generated code:\n",
      "```\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    for i in range(len(numbers) - 1):\n",
      "        for j in range(i + 1, len(numbers)):\n",
      "            if abs(numbers[i] - numbers[j]) < threshold:\n",
      "                return True\n",
      "    return False\n",
      "```\n",
      "\n",
      "-------------------------------------\n",
      "Cleaned generated code:\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    for i in range(len(numbers) - 1):\n",
      "        for j in range(i + 1, len(numbers)):\n",
      "            if abs(numbers[i] - numbers[j]) < threshold:\n",
      "                return True\n",
      "    return False\n",
      "\n",
      "----------------------------------\n",
      "Final completion after stripping function def:\n",
      "    for i in range(len(numbers) - 1):\n",
      "        for j in range(i + 1, len(numbers)):\n",
      "            if abs(numbers[i] - numbers[j]) < threshold:\n",
      "                return True\n",
      "    return False\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pass@1 samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Raw generated code:\n",
      "```\n",
      "def separate_paren_groups(paren_string):\n",
      "    stack = []\n",
      "    groups = []\n",
      "    for char in paren_string:\n",
      "        if char == \"(\":\n",
      "            stack.append(char)\n",
      "        elif char == \")\":\n",
      "            top = stack.pop()\n",
      "            while top != \"(\":\n",
      "                groups[-1] += top\n",
      "                top = stack.pop()\n",
      "    for group in stack:\n",
      "        groups.append(group[1:-1])\n",
      "    return groups\n",
      "```\n",
      "\n",
      "-------------------------------------\n",
      "Cleaned generated code:\n",
      "def separate_paren_groups(paren_string):\n",
      "    stack = []\n",
      "    groups = []\n",
      "    for char in paren_string:\n",
      "        if char == \"(\":\n",
      "            stack.append(char)\n",
      "        elif char == \")\":\n",
      "            top = stack.pop()\n",
      "            while top != \"(\":\n",
      "                groups[-1] += top\n",
      "                top = stack.pop()\n",
      "    for group in stack:\n",
      "        groups.append(group[1:-1])\n",
      "    return groups\n",
      "\n",
      "----------------------------------\n",
      "Final completion after stripping function def:\n",
      "    stack = []\n",
      "    groups = []\n",
      "    for char in paren_string:\n",
      "        if char == \"(\":\n",
      "            stack.append(char)\n",
      "        elif char == \")\":\n",
      "            top = stack.pop()\n",
      "            while top != \"(\":\n",
      "                groups[-1] += top\n",
      "                top = stack.pop()\n",
      "    for group in stack:\n",
      "        groups.append(group[1:-1])\n",
      "    return groups\n",
      "\n",
      "----------------------------------\n",
      "Raw generated code:\n",
      "return number % 1\n",
      "\n",
      "-------------------------------------\n",
      "Cleaned generated code:\n",
      "return number % 1\n",
      "\n",
      "----------------------------------\n",
      "Final completion after stripping function def:\n",
      "return number % 1\n",
      "\n",
      "\n",
      "Finished running limited problems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# ==============================\n",
    "# SETTINGS\n",
    "# ==============================\n",
    "\n",
    "model_name = \"codellama:7b\"\n",
    "pass_k = 1\n",
    "NUM_PROBLEMS = 3   # ðŸ”¥ Change this to control number of problems\n",
    "\n",
    "# ==============================\n",
    "# SELECT LIMITED PROBLEMS\n",
    "# ==============================\n",
    "\n",
    "problem_ids = list(problems.keys())[:NUM_PROBLEMS]\n",
    "limited_problems = {k: problems[k] for k in problem_ids}\n",
    "\n",
    "# ==============================\n",
    "# RUN GENERATION\n",
    "# ==============================\n",
    "\n",
    "samples = generate_pass_k_samples(\n",
    "    problems=limited_problems,\n",
    "    model_name=model_name,\n",
    "    k=pass_k\n",
    ")\n",
    "\n",
    "print(\"\\nFinished running limited problems.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f2bfeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pass@1 samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [04:23<00:00,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated samples saved to BASIC CODE RESULTS/samples_BASIC_codellama_7b_pass@1.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model_name = \"codellama:7b\"\n",
    "pass_k = 1   # ðŸ”¥ change this to 1, 2, 5, 10, etc.\n",
    "\n",
    "output_dir = \"BASIC CODE RESULTS\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "samples = generate_pass_k_samples(\n",
    "    problems=problems,\n",
    "    model_name=model_name,\n",
    "    k=pass_k\n",
    ")\n",
    "\n",
    "output_file = os.path.join(\n",
    "    output_dir,\n",
    "    f\"samples_BASIC_codellama_7b_pass@{pass_k}.jsonl\"\n",
    ")\n",
    "\n",
    "write_jsonl(output_file, samples)\n",
    "\n",
    "print(f\"Generated samples saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "637be8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 27188.37it/s]\n",
      "Running test suites...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:04<00:00, 37.21it/s]\n",
      "Writing results to BASIC CODE RESULTS/samples_BASIC_codellama_7b_pass@1.jsonl_results.jsonl...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:00<00:00, 41083.79it/s]\n",
      "{'pass@1': 0.3170731707317073}\n"
     ]
    }
   ],
   "source": [
    "! evaluate_functional_correctness \"BASIC CODE RESULTS/samples_BASIC_codellama_7b_pass@{pass_k}.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b2f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_passed_results(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSONL results file and extracts all entries where 'passed' is True.\n",
    "    \"\"\"\n",
    "    passed_results = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            if data.get(\"passed\", False):\n",
    "                passed_results.append(data)\n",
    "    return passed_results\n",
    "\n",
    "\n",
    "def print_passed_results(file_path, limit=5):\n",
    "    \"\"\"\n",
    "    Prints passed results (default: first 5).\n",
    "    \"\"\"\n",
    "    passed = get_passed_results(file_path)\n",
    "    print(f\"\\nTotal Passed Cases: {len(passed)}\")\n",
    "\n",
    "    for i, item in enumerate(passed[:limit]):\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"Task ID: {item['task_id']}\")\n",
    "        print(\"Completion:\\n\", item[\"completion\"])\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def print_failed_results(file_path, limit=5):\n",
    "    \"\"\"\n",
    "    Prints failed results (default: first 5).\n",
    "    \"\"\"\n",
    "    failed = get_failed_results(file_path)\n",
    "    print(f\"\\nTotal Failed Cases: {len(failed)}\")\n",
    "\n",
    "    for i, item in enumerate(failed[:limit]):\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"Task ID: {item['task_id']}\")\n",
    "        print(\"Completion:\\n\", item[\"completion\"])\n",
    "        print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09b577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Task ID: HumanEval/1\n",
      "Completion:\n",
      "     result = []\n",
      "    stack = []\n",
      "    for char in paren_string:\n",
      "        if char == \"(\":\n",
      "            stack.append(char)\n",
      "        elif char == \")\":\n",
      "            top = stack.pop()\n",
      "            while top != \"(\":\n",
      "                result.append(top)\n",
      "                top = stack.pop()\n",
      "    return result\n",
      "\n",
      "================================================================================\n",
      "Task ID: HumanEval/3\n",
      "Completion:\n",
      "      balance = 0\n",
      "     for operation in operations:\n",
      "         balance += operation\n",
      "         if balance < 0:\n",
      "             return True\n",
      "     return False\n",
      "\n",
      "================================================================================\n",
      "Task ID: HumanEval/4\n",
      "Completion:\n",
      "     mean_val = mean(numbers)\n",
      "    return sum(map(lambda x: abs(x - mean_val), numbers)) / len(numbers)\n",
      "\n",
      "================================================================================\n",
      "Task ID: HumanEval/5\n",
      "Completion:\n",
      "     return [delimeter if i < len(numbers) - 1 else n for i, n in enumerate(numbers)]\n",
      "\n",
      "================================================================================\n",
      "Task ID: HumanEval/6\n",
      "Completion:\n",
      "     result = []\n",
      "    for group in paren_string.split():\n",
      "        level = 0\n",
      "        for char in group:\n",
      "            if char == '(':\n",
      "                level += 1\n",
      "            elif char == ')':\n",
      "                level -= 1\n",
      "        result.append(level)\n",
      "    return result\n"
     ]
    }
   ],
   "source": [
    "results_file = \"BASIC CODE RESULTS/samples_BASIC_codellama_7b_pass@1.jsonl_results.jsonl\"\n",
    "\n",
    "print_passed_results(results_file, limit=5)\n",
    "print_failed_results(results_file, limit=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
